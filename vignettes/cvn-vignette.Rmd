---
title: "Covariate-varying Networks"
author: "Louis Dijkstra, Arne Godt, Ronja Foraita"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Covariate-varying Networks}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>",
  fig.width = 8,   # Adjust this to match the text width
  fig.height = 5  # Maintain a good aspect ratio
)
```

<!-- devtools::build_rmd("vignettes/cvn-vignette.Rmd") -->

Covariate-varying networks are high-dimensional dynamic graphical models that vary 
with multiple external covariates. The methodology is explained in detail
in the pre-print by [Dijkstra, Godt and Foraita (2024)](https://arxiv.org/abs/2407.19978).
In this vignette we describe how to apply the method to data with the associated
R package `CVN`.

A CVN is represented as graphical model with the quintupel 

$$
\text{CVN} = \{\mathbf{X}, \mathbf{U}, \mathcal{U}, f(\mathbf{X} \mid \mathbf{U}), 
\{G(u) = (V, E(u)) \}_{u \in \mathcal{U}} \},
$$
where $\mathbf{X} = (X_1, X_2 \ldots, X_p)^\top$ is a $p$-dimensional random vector and 
$\mathbf{U} = (U_1, U_2, \ldots, U_q)^\top$ is a random vector representing $q$ 
external discrete covariates. The external covariates are not included in the graph. 
These covariates have $(K_1,\ldots, K_q)^\top$ categories.

The vector $\mathbf{U}$ lies in the discrete space $\mathcal{U}$ with cardinality 
$m \leq \prod_{k=1}^q K_k$. The joint density function of $\mathbf{X}$ conditioned on 
$\mathbf{U}$ is $f(\mathbf{X} \mid \mathbf{U})$. 
The fifth element of the CVN is a set of $m$ graphs, one for each value $u$ in $\mathcal{U}$. 
The vertices of $G(u)$, $V = \{1, 2, \ldots, p\}$, correspond to the variables 
$X_1, X_2, \ldots, X_p$ and do not change with $\mathbf{U}$. 

We assume that $\mathbf{X} \mid \mathbf{U}$ follows a multivariate normal distribution
with $\mathbf{\mu}(u) = 0$ and covariance matrix $\mathbf{\Sigma}(u)$. 
To estimate the structure of the graph, we focus on the precision matrix 
$\mathbf{\Theta}(u) = \mathbf{\Sigma}(u)^{-1}$ which has the following property 
under the normality assumption:

$$
\theta_{ij}(u) = 0 \Leftrightarrow 
X_i \not\!\perp\!\!\!\perp X_j \mid \mathbf{X}_{V \setminus \{i,j\}} \land U = u  
\Leftrightarrow 
\text{edge } \{i,j\} \notin E(u)
$$
Hence, the (in)dependence structure of the CVN can be estimated by determining the zero entries of 
the precision matrices.
Our goal is to enable a CVN to handle high-dimensional data and identify structural similarities between graphs. Therefore, we define the CVN estimator of $\mathbf{ \Theta}_{\text{CVN}} = \{ \widehat{\mathbf{\Theta}}_i \}_{i = 1}^m$ 
as follows

$$ \mathbf{ \Theta}_{\text{CVN}} = \text{argmin}_{\{ \mathbf{\Theta}_{i}\}_{i = 1}^m} 
	\Bigg[ \sum_{i = 1}^m \ell\left({\mathbf{\Theta}_i}\right) 
	 + \lambda_1 \sum_{i = 1}^m \left\rVert \mathbf{\Theta}_i \right\rVert_1 
	 + \lambda_2 \sum_{i < j} w_{ij} \left\lVert \mathbf{\Theta}_i - \mathbf{\Theta}_j \right\rVert_1 \Bigg],$$


where $\mathbf{ \Theta}_i$ is the $i$-th precision matrix, $\ell\left({\mathbf{\Theta}_i}\right)$ the log-likelihood function for precision matrix $i$, $w_{ij}$ values of the symmetric
weighted adjacency matrix of the meta-graph (see below) and $\lambda_1, \lambda_2$ are
two tuning parameters. 



## Data
Suppose you have two external variables $U_1$ and $U_2$, each having $K = L = 3$ categories. 
The input of a CVN is then a list of $K \cdot L = m$ data sets. 
The number of variables $p$ must be equal in each list element, the number of observations may differ.



```{r data, warning=FALSE}
# Load required library
library(CVN)
library(dplyr)


# Load data: a list of 9 data sets with p variables an n_i observations.
# data(grid)
# m <- length(grid)

# Simulate the dataset
set.seed(2024)  
n <- 300  

# Create 10 normally distributed variables for the graph
data <- as.data.frame(matrix(rnorm(n * 10), ncol = 10))
colnames(data) <- paste0("X", 1:10)

# Add two discrete external covariates
data$dosis <- sample(1:3, n, replace = TRUE)  
data$bmi <- sample(c("normal", "overweight"), n, replace = TRUE)

# Split the dataset into subsets based on dosis and bmi
data_list <- data %>%
  group_by(dosis, bmi) %>%
  group_split() %>%
  lapply(function(df) df %>% select(-dosis, -bmi))

names(data_list) <- apply(expand.grid(dosis = 1:3, bmi = c("normal", "overweight")), 1, 
                               function(x) paste0("dosis_", x[1], "_", x[2]))

```
## Weigth matrix

The weight matrix is a $m \times m$ symmetric matrix. It reflects the *meta-graph* which
encodes the smoothing structure between the graphs. A weight of 0 does not smooth 
the structure between two graphs. Weights must be chosen between 0 and 1.  

In case of two external covariates, the weight matrix 
can be generated using a simple function. 

```{r W}
W_full <- create_weight_matrix("full", k = 3, l = 2)  
# plots can be turned on by setting plot = TRUE. You need the igraph package for this
W_grid <- create_weight_matrix("grid", k = 3, l = 3, plot = TRUE)
W_gl   <- create_weight_matrix("glasso", k = 3, l = 2)
W_random <- round(create_weight_matrix("uniform-random", k = 4, l = 3), 2)
```

Weight matrices can be plotted as a grid graph or as a heatmap:
```{r W-heat}
plot_weight_matrix(W_random, k = 4, l = 3)
hd_weight_matrix(W_random)  # randomly chosen weights
```




## Tuning parameter space

A CVN requires to select two tuning parameters which control the regularization applied
to the CVN. These parameters are usually searched for in a predetermined regularization path.
$\lambda_1$ introduces sparsity in the CVN, which gets sparser the larger $\lambda_1$ is 
chosen. $\lambda_2$ penalizes differences between the precision matrices that are 
connected in the meta-graph. It encourages similar values in the precision
matrices and hence also if an edge is included in the graph or not. The graphs in the CVN
are getting more and more similar the larger $\lambda_2$ is selected. 

```{r tp}
# Lets choose a regularization path for each tuning parameter
lambda1 = seq(0.5, 2, length = 3)  # sparsity
lambda2 = c(1, 1.5)                # smoothing
```


## Estimate a CVN

The CVN is simply estimated by the following. It will parallelize by default if there are
more than one $\lambda_1$ or $\lambda_2$ values.  

```{r cvn}
fit <- CVN::CVN(data = grid, 
                W = W_grid, 
                lambda1 = lambda1, 
                lambda2 = lambda2, 
                eps = 1e-2, maxiter = 500, 
                verbose = FALSE,
                n_cores = 1)

print(fit)
```

It is also possible to change the $\gamma$-value for the eBIC. The default is $\gamma = 0.5$.

```{r ebic}
(dic <- determine_information_criterion_cvn(fit, gamma = 0.9))

cat("minimal AIC: ", which.min(fit$res$aic))  
cat("\nminimal BIC: ", which.min(fit$res$bic))  
cat("\nminimal eBIC with gamma = 0.5: ", which.min(fit$res$ebic))
cat("\nminimal eBIC with gamma = 0.9: ", which.min(dic[3,]))  
```
Based on the information criteria, it is unclear which graph to choose. The AIC 
selects slightly denser models than the BIC. Our simulation study showed that 
AIC tends to perform better than BIC in general, 
but all perform rather poorly in practice.

The package also includes a plot function to check if the regularization path is too small.
The yellow dot shows the tuning parameter constellation that minimizes the BIC. 
If this point is on the limit, you should think about to choose different tuning parameters.

```{r plotIC}
plot_information_criterion(fit, use_gammas = FALSE, criterion = "bic")
```


In the following, however, we want to investigate the CVN with id = 6. 
It is easier to extract that CVN into a new object.

```{r extract}
fit3 <- extract_cvn(fit, 3)
```


# Plot a CVN model
The graphs can be plotted with a print function that relies on the `visnet`package.

```{r pp, eval=FALSE}

plot_cvn <- visnetwork_cvn(fit6, verbose = FALSE)

## DER CORE GRAPH WIRD FALSCH GEFÃ„RBT

```

Differences between the graphs can be investigated looking at the number of edges in each
CVN subgraph and by investigating the Hamming distances between the subgraphs. 


```{r hamming, eval=FALSE}
# Shows the number of edges in each subgraph, the number of edges that are shared by 
# all graphs (core) and the number of edges that are unique for each subgraph
cvn_edge_summary(fit)

# Hamming distance matrix and plot
hd_matrix <- hamming_distance_adj_matrices(fit3$adj_matrices[[1]]) 
plot_hamming_distances(hd_matrix)

# or directly
# plot_hamming_distances_cvn(fit3)

```

# Interpolate graph based an previously fitted CVN model

Suppose we have graphs for 3 time points and we want to interpolate the graph for
the 4th time point based on a previously fitted model. We can do this as follows.


```{r interpolate}
set.seed(86)
time_data <- lapply(1:3, function(x) matrix(rnorm(500), ncol = 5))
W <- create_weight_matrix("grid", 3, 1)
timefit <- CVN(time_data, W, maxiter = 1000,
               lambda1 = 6, lambda2 = .5, verbose = FALSE)

timefit$adj_matrices
timefit4 <- interpolate(timefit, c(0,0.25,1), truncate = 1e-02)
timefit4$adj_matrices
```
The function is still work in progress and applying it to real data requires more experience. 
We also did not investigate its performance in simulation study. 
It should therefore be used with caution.  

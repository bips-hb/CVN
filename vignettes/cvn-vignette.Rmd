---
title: "Covariate-varying Networks"
author: "Louis Dijkstra, Arne Godt, Ronja Foraita"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{my-vignette}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```

<!-- devtools::build_rmd("vignettes/cvn-vignette.Rmd") -->

Covariate-varying networks are high-dimensional dynamic graphical models that vary 
with multiple external covariates. The methodology is explained in detail
in the pre-print by [Dijkstra, Godt and Foraita (2024)](https://arxiv.org/abs/2407.19978).
In this vignette we describe how to apply the method to data with the associated
R package `CVN`.

A CVN is represented as graphical model with the quintupel 

$$
\text{CVN} = \{\mathbf{X}, \mathbf{U}, \mathcal{U}, f(\mathbf{X} \mid \mathbf{U}), 
\{G(u) = (V, E(u)) \}_{u \in \mathcal{U}} \},
$$
where $\mathbf{X} = (X_1, X_2 \ldots, X_p)^\top$ is a $p$-dimensional random vector and 
$\mathbf{U} = (U_1, U_2, \ldots, U_q)^\top$ is a random vector representing $q$ 
external discrete covariates. The external covariates are not included in the graph. 
These covariates have $(K_1,\ldots, K_q)^\top$ categories.

The vector $\mathbf{U}$ lies in the discrete space $\mathcal{U}$ with cardinality 
$m \leq \prod_{k=1}^q K_k$. The joint density function of $\mathbf{X}$ conditioned on 
$\mathbf{U}$ is $f(\mathbf{X} \mid \mathbf{U})$. 
The fifth element of the CVN is a set of $m$ graphs, one for each value $u$ in $\mathcal{U}$. 
The vertices of $G(u)$, $V = \{1, 2, \ldots, p\}$, correspond to the variables 
$X_1, X_2, \ldots, X_p$ and do not change with $\mathbf{U}$. 

We assume that $\mathbf{X} \mid \mathbf{U}$ follows a multivariate normal distribution
with $\mathbf{\mu}(u) = 0$ and covariance matrix $\mathbf{\Sigma}(u)$. 
To estimate the structure of the graph, we focus on the precision matrix 
$\mathbf{\Theta}(u) = \mathbf{\Sigma}(u)^{-1}$ which has the following property 
under the normality assumption:

$$
\theta_{ij}(u) = 0 \Leftrightarrow 
X_i \not\!\perp\!\!\!\perp X_j \mid \mathbf{X}_{V \setminus \{i,j\}} \land U = u  
\Leftrightarrow 
\text{edge } \{i,j\} \notin E(u)
$$
Hence, the (in)dependence structure of the CVN can be estimated by determining the zero entries of 
the precision matrices.
Our goal is to enable a CVN to handle high-dimensional data and identify structural similarities between graphs. Therefore, we define the CVN estimator of $\mathbf{ \Theta}_{\text{CVN}} = \{ \widehat{\mathbf{\Theta}}_i \}_{i = 1}^m$ 
as follows

$$ \mathbf{ \Theta}_{\text{CVN}} = \text{argmin}_{\{ \mathbf{\Theta}_{i}\}_{i = 1}^m} 
	\Bigg[ \sum_{i = 1}^m \ell\left({\mathbf{\Theta}_i}\right) 
	 + \lambda_1 \sum_{i = 1}^m \left\rVert \mathbf{\Theta}_i \right\rVert_1 
	 + \lambda_2 \sum_{i < j} w_{ij} \left\lVert \mathbf{\Theta}_i - \mathbf{\Theta}_j \right\rVert_1 \Bigg],$$


where $\mathbf{ \Theta}_i$ is the $i$-th precision matrix, $\ell\left({\mathbf{\Theta}_i}\right)$ the log-likelihood function for precision matrix $i$, $w_{ij}$ values of the symmetric
weighted adjacency matrix of the meta-graph (see below) and $\lambda_1, \lambda_2$ are
two tuning parameters. 



## Data
Suppose you have two external variables $U_1$ and $U_2$, each having $K = L = 3$ categories. 
The input of a CVN is then a list of $K \cdot L = m$ data sets. 
The number of variables $p$ must be equal in each list element, the number of observations may differ.



```{r data, warning=FALSE}
# Load required library
library(CVN)
library(dplyr)


# Load data: a list of 9 data sets with p variables an n_i observations.
# data(grid)
# m <- length(grid)

# Simulate the dataset
set.seed(2024)  
n <- 300  

# Create 10 normally distributed variables for the graph
data <- as.data.frame(matrix(rnorm(n * 10), ncol = 10))
colnames(data) <- paste0("X", 1:10)

# Add two discrete external covariates
data$dosis <- sample(1:3, n, replace = TRUE)  
data$bmi <- sample(c("normal", "overweight"), n, replace = TRUE)

# Split the dataset into subsets based on dosis and bmi
data_list <- data %>%
  group_by(dosis, bmi) %>%
  group_split() %>%
  lapply(function(df) df %>% select(-dosis, -bmi))

names(data_list) <- apply(expand.grid(dosis = 1:3, bmi = c("normal", "overweight")), 1, 
                               function(x) paste0("dosis_", x[1], "_", x[2]))

```
## Weigth matrix

The weight matrix is a $m \times m$ symmetric matrix. It reflects the *meta-graph* which
encodes the smoothing structure between the graphs. A weight of 0 does not smooth 
the structure between two graphs. Weights must be chosen between 0 and 1.  

In case of two external covariates, the weight matrix 
can be generated using a simple function. 

```{r W, eval=FALSE}
W_full <- create_weight_matrix("full", k = 3, l = 2)  
W_grid <- create_weight_matrix("grid", k = 3, l = 3)
# plots can be turned off by setting plot = FALSE
W_gl   <- create_weight_matrix("glasso", k = 3, l = 2, plot = FALSE)
W_random <- round(create_weight_matrix("uniform-random", k = 4, l = 3, plot = FALSE), 2)
```

Weight matrices can be plotted as a grid graph or as a heatmap:
```{r W-heat}
plot_weight_matrix(W_random, k = 4, l = 3)
hd_weight_matrix(W_random)  # randomly chosen weights
```




## Tuning parameter space

A CVN requires to select two tuning parameters which control the regularization applied
to the CVN. These parameters are usually searched for in a predetermined regularization path.
$\lambda_1$ introduces sparsity in the CVN, which gets sparser the larger $\lambda_1$ is 
chosen. $\lambda_2$ penalizes differences between the precision matrices that are 
connected in the meta-graph. It encourages similar values in the precision
matrices and hence also if an edge is included in the graph or not. The graphs in the CVN
are getting more and more similar the larger $\lambda_2$ is selected. 

```{r tp}
# Lets choose a regularization path for each tuning parameter
lambda1 = seq(0.5, 2, length = 3)  # sparsity
lambda2 = c(0.5, 1)                # smoothing
```


## Estimate a CVN

The CVN is simply estimated by the following. It will parallelize by default if there are
more than one $\lambda_1$ or $\lambda_2$ values.  

```{r cvn, eval=FALSE}
fit <- CVN::CVN(data = grid, 
                W = W_grid, 
                lambda1 = lambda1, 
                lambda2 = lambda2, 
                eps = 1e-2, maxiter = 500, 
                verbose = FALSE,
                n_cores = 1)

print(fit)
```

It is also possible to change the $\gamma$-value for the eBIC. The default is $\gamma = 0.5$.

```{r ebic}
(dic <- determine_information_criterion_cvn(fit, gamma = 0.9))

which.min(fit$res$aic)  # minimal AIC
which.min(fit$res$bic)  # minimal BIC
which.min(fit$res$ebic) # minimal eBIC with gamma = 0.5
which.min(dic[3,])      # minimal eBIC with gamma = 0.9
```
Based on all information criteria, the final model is preferred over the others. Additionally, it exhibits the sparsest network when averaged across all 9 graphs. However, our simulation study revealed that information criteria tend to perform poorly in practice.


# Print and Plots
The print function in our package uses the `visnet`package.

```{r pp, eval=FALSE}
# I only want to print the 6th CVN
fit6 <- extract_cvn(fit, 6)
plot_cvn <- visnetwork_cvn(fit6, verbose = FALSE)

## DER CORE GRAPH WIRD FALSCH GEFÃ„RBT


# igraph
#plot_graph(fit)

core_graph <- find_core_graph(fit6)

```



```{r hamming, eval=FALSE}
hamming_distance_adj_matrices(cvn$adj_matrices[[best.aic]]) 
hamming_distance_adj_matrices(cvn$adj_matrices[[best.bic]]) 

plot_hamming_distances_cvn(cvn)

```

# Interpolate